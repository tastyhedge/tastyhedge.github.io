<!DOCTYPE html>
<html lang="en-US">

<head>
  <title>Pricing Derivatives on a Budget | TastyHedge</title>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Oleksandr Gituliar">
  <meta name="description"
    content="After five years working as a quant, I can tell that the wast majority of derivative pricing in the financial industry is done on CPU. This is easily explained by two facts: (1) no GPU was available when banks started developing their pricing analytics in 90&rsquo;s; and (2) banking is a conservative sector, slow to upgrade its technical stack.
American Options. In this post, I benchmark pricing of American Options on GPU." />

  
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-HZRVS8S0KQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-HZRVS8S0KQ');
  </script>
  

  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/css/code.css">
  <link rel="icon" href="/favicon.ico">

  <meta property="og:title" content="Pricing Derivatives on a Budget" />
<meta property="og:description" content="After five years working as a quant, I can tell that the wast majority of derivative pricing in the financial industry is done on CPU. This is easily explained by two facts: (1) no GPU was available when banks started developing their pricing analytics in 90&rsquo;s; and (2) banking is a conservative sector, slow to upgrade its technical stack.
American Options. In this post, I benchmark pricing of American Options on GPU." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tastyhedge.com/blog/pricing-derivatives-on-a-budget/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2023-10-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-10-01T00:00:00+00:00" />



  
</head>

<body class="mx-auto bg-stone-200">
  <div class="bg-white pb-1 min-h-screen px-4 md:px-12 border-x-[1px] border-black">
    <nav class="flex justify-between pb-6">
      <a class="block font-bold my-auto text-2xl tracking-tighter text-inherit no-underline"
        style="text-underline-offset: 6px;" href="/">
        <img src="/img/logo/logo-32x32.png" class="inline-flex py-4 me-2 w-8"><span
          class="align-middle">TastyHedge</span>
      </a>
      <div class=" my-auto text-xl">
        <a class="text-black" href="/about">About</a>
      </div>
    </nav>

    

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="" itemprop="name headline">Pricing Derivatives on a Budget</h1>
    

    <p class="mb-0 flex justify-between">
      <span class="my-auto">
        <time class="dt-published" datetime='2023-10-01 00:00:00 &#43;0000 UTC' itemprop="datePublished">
          2023-10-01
        </time>
        â€”
        <span itemprop="author" itemscope itemtype="http://schema.org/Person">
          <a href="/about" itemprop="name">Oleksandr Gituliar</a></span>
      </span>
      <span>
        Follow on
        <a href="https://www.linkedin.com/mynetwork/discovery-see-all/?usecase=PEOPLE_FOLLOWS&followMember=gituliar"
          target="_blank" title="LinkedIn" class="social-media no-underline">
          <img src="/img/brands/linkedin.svg" />
        </a>
        <a href="https://github.com/gituliar" target="_blank" title="GitHub" class="social-media no-underline">
          <img src="/img/brands/github.svg" />
        </a>
        
      </span>
    </p>
  </header>

  <div class="article-body" itemprop="articleBody">
    <p>After five years working as a quant, I can tell that the wast majority of derivative pricing in the
financial industry is done on CPU. This is easily explained by two facts: (1) no GPU was available
when banks started developing their pricing analytics in 90&rsquo;s; and (2) banking is a conservative
sector, slow to upgrade its technical stack.</p>
<p><strong>American Options.</strong> In this post, I benchmark pricing of American Options on GPU. Since no
analytical formula exist to price American options (similar to the Black-Scholes formula for
European options), people in banks use numerical methods to solve this sort of problems. Such
methods are computationally greedy and, in practice, require a lot of hardware to risk-manage
trading books with thousands of positions.</p>
<p><strong>Finite Difference.</strong> For the benchmark, I use my own implementations of the
<a href="https://en.wikipedia.org/wiki/Finite_difference_method">finite-difference method</a> for CPU and GPU.
When it comes to pricing derivatives, there are two methods, widely-adopted by the industry, that
are capable to solve a wide range of pricing problems. The first one is the famous <strong>Monte-Carlo</strong>
method. Another one is the <strong>Finite-Difference</strong> method, which we will focus on in this post today.</p>
<p>Note, a <!-- raw HTML omitted -->much faster method<!-- raw HTML omitted --> to price American Options was recently developed by Andersen et
al. (details below). This method has some constraints (like time-independent coefficients or
log-normal underlying process) and is not a complete replacement for the finite-difference.</p>
<p><strong>Source Code.</strong> The code is written in C++ / CUDA and is available on Github:
<a href="https://github.com/gituliar/kwinto-cuda">https://github.com/gituliar/kwinto-cuda</a>. It&rsquo;s compatible with Linux and Windows, but requires
Nvidia GPU.</p>
<p>The finite-difference algorithm itself is not very complicated, however deserves a <!-- raw HTML omitted -->dedicated
post<!-- raw HTML omitted --> to be fully explained, as there are some nuances here and there. Hopefully, I&rsquo;ll find some
time to cover this topic later.</p>
<p><strong>Main focus</strong> of the benchmarking is on the following:</p>
<ul>
<li>How much <!-- raw HTML omitted -->faster<!-- raw HTML omitted --> is GPU vs CPU? <!-- raw HTML omitted --> Speed is a convenient metric to compare performance,
as faster usually means better (given all other factors equal).</li>
<li>How much <!-- raw HTML omitted -->cheaper<!-- raw HTML omitted --> is GPU vs CPU? <!-- raw HTML omitted --> Budget is always important when making decisions.
Speed matters because fast code means less CPU time, but there are other essential factors worth
discussing that impact your budget.</li>
</ul>
<h2 id="benchmark">Benchmark</h2>
<p>My approach is to price american options <!-- raw HTML omitted -->in batches<!-- raw HTML omitted -->. This is usually how things are run in
banks, when risk-managing trading books. Every batch contains from 256 to 16'384 american options,
which are priced in parallel utilizing <!-- raw HTML omitted -->all cores<!-- raw HTML omitted --> on CPU or GPU.</p>
<p>The total pool of 378'000 options for the benchmark is constructed by permuting all combinations of
the following parameters (with options cheaper than 0.5 rejected). <!-- raw HTML omitted -->Reference prices<!-- raw HTML omitted --> are
calculated with <a href="https://github.com/gituliar/kwinto-cuda/blob/main/test/portfolio.py">portfolio.py</a>
in QuantLib, using the
<a href="https://hpcquantlib.wordpress.com/2022/10/09/high-performance-american-option-pricing/">Spanderen implementation</a>
of a high-performance
<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2547027">Andersen-Lake-Offengenden algorithm</a>
for pricing american options.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Range</th>
</tr>
</thead>
<tbody>
<tr>
<td>Parity</td>
<td>PUT</td>
</tr>
<tr>
<td>Strike</td>
<td>100</td>
</tr>
<tr>
<td>Spot</td>
<td>25, 50, 80, 90, 100, 110, 120, 150, 175, 200</td>
</tr>
<tr>
<td>Maturity</td>
<td>1/12, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0</td>
</tr>
<tr>
<td>Volatility</td>
<td>0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5</td>
</tr>
<tr>
<td>Interest rate</td>
<td>1%, 2%, 3%, 4%, 5%, 6%, 7%, 8%, 9%, 10%</td>
</tr>
<tr>
<td>Dividend rate</td>
<td>0%, 2%, 4%, 6%, 8%, 10%, 12%</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>Results.</strong> A plot below depicts the main results. Each bin shows how many options are priced per
second (higher is better):</p>
<p><img src="/img/2023-10-01/bench-512-cpu-gpu.png" alt="Benchmark CPU vs GPU"></p>
<p><strong>US Options Market.</strong> To get some idea about how these results are useful in practice, let&rsquo;s have a
look at a size of the US options market. The data from <a href="https://www.opraplan.com">OPRA</a> tells that
there are:</p>
<ul>
<li>5'800 stocks</li>
<li>680'000 options (with 5%-95% delta)</li>
</ul>
<p>In other words, as per the benchmark, it takes <strong>2 min</strong> to price an entire US Options Market on a
$100 GPU. It should take 10x longer for calibration, which is a more challenging task.</p>
<p>Few things to keep in mind for the results above:</p>
<ol>
<li>
<p><strong>GPU is 2x faster</strong> in a <!-- raw HTML omitted -->single-precision<!-- raw HTML omitted --> mode (gray bin) vs double-precision (yellow
bin). Meantime, CPU performs more or less the same in both modes (blue and orange bins).</p>
<p>This is a clear sign that the GPU is limited by <!-- raw HTML omitted -->data throughput<!-- raw HTML omitted -->. With its 1'920 cores, the
GPU processes data faster than loads it from the GPU memory.</p>
</li>
<li>
<p><strong>GPU is 4 years older</strong>, which is a big gap for hardware. Nevertheless, the oldish GPU is still
faster than the modern CPU.</p>
<ul>
<li><a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1070.c2840">Nvidia GTX 1070</a>, 16nm &ndash;
released in 2016.</li>
<li><a href="https://www.techpowerup.com/cpu-specs/ryzen-9-5900x.c2363">AMD Ryzen 9 X5900</a>, 7nm &ndash; released
in 2020.</li>
</ul>
</li>
</ol>
<!-- raw HTML omitted -->
<h2 id="budget">Budget</h2>
<p>In the production environment, <!-- raw HTML omitted -->faster<!-- raw HTML omitted --> is almost always means <!-- raw HTML omitted -->cheaper<!-- raw HTML omitted -->. Speed is not the
only factor that affects operational costs. Let&rsquo;s take a look at other factors that appeal in favor
of GPU:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>Cheap to scale.</strong> To run an <!-- raw HTML omitted -->extra CPU<!-- raw HTML omitted --> it requires a motherboard, RAM, HDD &ndash; a whole new
machine, which quickly becomes expensive to scale.</p>
<p>An <!-- raw HTML omitted -->extra GPU<!-- raw HTML omitted -->, however, requires only a PCI-E slot. Some motherboards offer a dozen PCI-E
ports, like <a href="https://www.asrock.com/mb/Intel/Q270%20Pro%20BTC+/index.asp">ASRock Q270 Pro BTC+</a>,
which is especially popular among crypto miners. Such a motherboard can handle <!-- raw HTML omitted -->17 GPUs on a
single machine<!-- raw HTML omitted -->. In addition, there is no need to setup a network, manage software on various
machines, and hire an army of devops to automate all that.</p>
<p>This gives extra 3-5x cost reduction in favor of GPU.</p>
<p><strong>Cheap to upgrade.</strong> PCI-E standard is backward compatible, so that new GPU cards are still run on
<!-- raw HTML omitted -->old machines<!-- raw HTML omitted -->. Below is the same benchmark run on a much older machine with dual
<a href="https://www.techpowerup.com/cpu-specs/xeon-x5675.c949">Xeon X5675</a> from 2011:</p>
<p><img src="/img/2023-10-01/bench-z800.png" alt="Benchmark CPU vs GPU"></p>
<p>What immediately catches the eye is that Ryzen 9 outperforms the dual-Xeon machine (both have equal
number of physical cores, btw). This is not a surprise, given a 10-year technological gap.</p>
<p>Surprising is that a newer <!-- raw HTML omitted -->GPU performs equally well<!-- raw HTML omitted --> on a much older machine. In practice,
this means that at some point in the future when GPU cards deserve an upgrade there is no need to
upgrade other components, like CPU, motherboard, etc.</p>
<h2 id="summary">Summary</h2>
<p>The <!-- raw HTML omitted -->Finite-Difference method<!-- raw HTML omitted --> is a universal and powerful method, heavily used in the
financial industry. However, what is the benefit of running it on GPU ?</p>
<p><strong>Final verdict.</strong> My benchmark shows that:</p>
<ul>
<li>GPU is 2x faster</li>
<li>GPU is 3x-5x cheaper</li>
</ul>
<p>This combined gives <!-- raw HTML omitted -->10x factor<!-- raw HTML omitted --> in favor of GPU as a platform for pricing derivatives.</p>

  </div>

  
  <div class="px-2">
    <script src="https://giscus.app/client.js" data-repo="gituliar/tastyhedge.com" data-repo-id="R_kgDOLP4lzg"
      data-category="General" data-category-id="DIC_kwDOLP4lzs4CdfL9" data-mapping="pathname" data-strict="0"
      data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast"
      data-lang="en" crossorigin="anonymous" async>
      </script>
  </div>
  
</article>


  </div>

  <script>
    document.querySelectorAll('a[href^="http"]').forEach(function (e) {
      e.setAttribute('target', '_blank');
    });
  </script>
</body>

</html>